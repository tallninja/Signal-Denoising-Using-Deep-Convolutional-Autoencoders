{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pesq import pesq\n",
    "from metrics import AudioMetrics, AudioMetrics2\n",
    "from audio_utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows the output of plotting commands to be displayed directly within the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# ignore depracation warnings\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed for random number generation in NumPy\n",
    "# Setting the seed ensures that the sequence of random \n",
    "# numbers generated will be the same every time the code is run.\n",
    "np.random.seed(999)\n",
    "\n",
    "# set the seed for random number generation in PyTorch\n",
    "torch.manual_seed(999)\n",
    "\n",
    "# If running on Cuda set these 2 for determinism\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Check GPU is available\n",
    "GPU_AVAILABLE=torch.cuda.is_available()\n",
    "\n",
    "if(GPU_AVAILABLE):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')\n",
    "       \n",
    "DEVICE = torch.device('cuda' if GPU_AVAILABLE else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 48000\n",
    "N_FFT = (SAMPLE_RATE * 64) // 1000          # 3072\n",
    "HOP_LENGTH = (SAMPLE_RATE * 16) // 1000     # 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_INPUT_DIR = Path('datasets/class_9_train_input')\n",
    "TRAIN_TARGET_DIR = Path('datasets/clean_trainset_28spk_wav')\n",
    "TEST_INPUT_DIR = Path('datasets/class_9_test_input')\n",
    "TEST_TARGET_DIR = Path('datasets/clean_testset_wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset class with audio that cuts them/paddes them to a specified length, applies a STFT,\n",
    "    normalizes and leads to a tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, clean_files, noisy_files, n_fft=64, hop_length=16):\n",
    "        super().__init__()\n",
    "        self.clean_files = sorted(clean_files)\n",
    "        self.noisy_files = sorted(noisy_files)\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.max_len = 165000\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.noisy_files)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # load waf files to waveform tensors\n",
    "        x_clean = self.load_sample(self.clean_files[index])\n",
    "        x_noisy = self.load_sample(self.noisy_files[index])\n",
    "\n",
    "        # apply padding and cutting\n",
    "        x_clean = self._prepare_sample(x_clean)\n",
    "        x_noisy = self._prepare_sample(x_noisy)\n",
    "\n",
    "        # apply STFT\n",
    "        x_clean_stft = torch.stft(input=x_clean, n_fft=self.n_fft, \n",
    "                                  hop_length=self.hop_length, normalized=True, return_complex=True)\n",
    "        x_noisy_stft = torch.stft(input=x_noisy, n_fft=self.n_fft,\n",
    "                                   hop_length=self.hop_length, normalized=True, return_complex=True)\n",
    "        x_clean_stft = torch.view_as_real(x_clean_stft)\n",
    "        x_noisy_stft = torch.view_as_real(x_noisy_stft)\n",
    "\n",
    "        return x_clean_stft, x_noisy_stft\n",
    "\n",
    "    \n",
    "    def load_sample(self, file):\n",
    "        waveform, _ = torchaudio.load(file)\n",
    "        return waveform\n",
    "    \n",
    "    def _prepare_sample(self, waveform):\n",
    "        \"\"\"\"\n",
    "        Processes an input waveform, which is a 2D NumPy array\n",
    "        \"\"\"\n",
    "        # converts an input waveform from a PytorchTensor to a NumPy array\n",
    "        waveform = waveform.numpy()\n",
    "        # get the length of the audio sequence\n",
    "        current_len = waveform.shape[1]\n",
    "        # initialize the output array with zeros\n",
    "        output = np.zeros((1, self.max_len), dtype='float32')\n",
    "        # copy the values from the input waveform into the initialized \n",
    "        # output array. It ensures that the waveform is placed at the end of the output \n",
    "        # array, and if the waveform is longer than self.max_len, it is truncated to fit.\n",
    "        output[0, -current_len:] = waveform[0, :self.max_len]\n",
    "        # convert the NumPy array to a PyTorch tensor\n",
    "        return torch.from_numpy(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_files = sorted(list(TRAIN_INPUT_DIR.rglob('*.wav')))\n",
    "train_target_files = sorted(list(TRAIN_TARGET_DIR.rglob('*.wav')))\n",
    "test_input_files = sorted(list(TEST_INPUT_DIR.rglob('*.wav')))\n",
    "test_target_files = sorted(list(TEST_TARGET_DIR.rglob('*.wav')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Input Files: 11572\n",
      "Train Target Files: 11572\n",
      "Test Input Files: 824\n",
      "Test Target Files: 824\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Input Files: {len(train_input_files)}\")\n",
    "print(f\"Train Target Files: {len(train_target_files)}\")\n",
    "print(f\"Test Input Files: {len(test_input_files)}\")\n",
    "print(f\"Test Target Files: {len(test_target_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SpeechDataset(train_target_files, train_input_files, N_FFT, HOP_LENGTH)\n",
    "test_dataset = SpeechDataset(test_target_files, test_input_files, N_FFT, HOP_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_set_metrics(test_loader, model):\n",
    "    metric_names = [\"CSIG\",\"CBAK\",\"COVL\",\"PESQ\",\"SSNR\",\"STOI\"]\n",
    "    overall_metrics = [[] for i in range(len(metric_names))]\n",
    "    \n",
    "    for i,(noisy,clean) in enumerate(test_loader):\n",
    "        x_est = model(noisy.to(DEVICE), is_istft=True)\n",
    "        x_est_np = x_est[0].view(-1).detach().cpu().numpy()\n",
    "        clean_cmplx = torch.view_as_complex(clean[0])\n",
    "        x_c_np = torch.istft(torch.squeeze(clean_cmplx, 1), n_fft=N_FFT, hop_length=HOP_LENGTH, normalized=True).view(-1).detach().cpu().numpy()\n",
    "        audio_metrics = AudioMetrics(x_c_np, x_est_np, SAMPLE_RATE)\n",
    "        \n",
    "        overall_metrics[0].append(audio_metrics.CSIG)\n",
    "        overall_metrics[1].append(audio_metrics.CBAK)\n",
    "        overall_metrics[2].append(audio_metrics.COVL)\n",
    "        overall_metrics[3].append(audio_metrics.PESQ)\n",
    "        overall_metrics[4].append(audio_metrics.SSNR)\n",
    "        overall_metrics[5].append(audio_metrics.STOI)\n",
    "    \n",
    "    metrics = dict()\n",
    "    for i in range(len(metric_names)):\n",
    "        metrics[metric_names[i]] ={'mean': np.mean(overall_metrics[i]), 'std_dev': np.std(overall_metrics[i])} \n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Class of complex valued convolutional layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.real_conv = nn.Conv2d(in_channels=self.in_channels, \n",
    "                                   out_channels=self.out_channels, \n",
    "                                   kernel_size=self.kernel_size, \n",
    "                                   padding=self.padding, \n",
    "                                   stride=self.stride)\n",
    "        \n",
    "        self.im_conv = nn.Conv2d(in_channels=self.in_channels, \n",
    "                                 out_channels=self.out_channels, \n",
    "                                 kernel_size=self.kernel_size, \n",
    "                                 padding=self.padding, \n",
    "                                 stride=self.stride)\n",
    "        \n",
    "        # Glorot initialization.\n",
    "        nn.init.xavier_uniform_(self.real_conv.weight)\n",
    "        nn.init.xavier_uniform_(self.im_conv.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_real = x[..., 0]\n",
    "        x_im = x[..., 1]\n",
    "        \n",
    "        c_real = self.real_conv(x_real) - self.im_conv(x_im)\n",
    "        c_im = self.im_conv(x_real) + self.real_conv(x_im)\n",
    "        \n",
    "        output = torch.stack([c_real, c_im], dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CConvTranspose2d(nn.Module):\n",
    "    \"\"\"\n",
    "      Class of complex valued dilation convolutional layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, output_padding=0, padding=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.output_padding = output_padding\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.real_convt = nn.ConvTranspose2d(in_channels=self.in_channels, \n",
    "                                            out_channels=self.out_channels, \n",
    "                                            kernel_size=self.kernel_size, \n",
    "                                            output_padding=self.output_padding,\n",
    "                                            padding=self.padding,\n",
    "                                            stride=self.stride)\n",
    "        \n",
    "        self.im_convt = nn.ConvTranspose2d(in_channels=self.in_channels, \n",
    "                                            out_channels=self.out_channels, \n",
    "                                            kernel_size=self.kernel_size, \n",
    "                                            output_padding=self.output_padding, \n",
    "                                            padding=self.padding,\n",
    "                                            stride=self.stride)\n",
    "        \n",
    "        \n",
    "        # Glorot initialization.\n",
    "        nn.init.xavier_uniform_(self.real_convt.weight)\n",
    "        nn.init.xavier_uniform_(self.im_convt.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_real = x[..., 0]\n",
    "        x_im = x[..., 1]\n",
    "        \n",
    "        ct_real = self.real_convt(x_real) - self.im_convt(x_im)\n",
    "        ct_im = self.im_convt(x_real) + self.real_convt(x_im)\n",
    "        \n",
    "        output = torch.stack([ct_real, ct_im], dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBatchNorm2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Class of complex valued batch normalization layer\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.affine = affine\n",
    "        self.track_running_stats = track_running_stats\n",
    "        \n",
    "        self.real_b = nn.BatchNorm2d(num_features=self.num_features, eps=self.eps, momentum=self.momentum,\n",
    "                                      affine=self.affine, track_running_stats=self.track_running_stats)\n",
    "        self.im_b = nn.BatchNorm2d(num_features=self.num_features, eps=self.eps, momentum=self.momentum,\n",
    "                                    affine=self.affine, track_running_stats=self.track_running_stats) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_real = x[..., 0]\n",
    "        x_im = x[..., 1]\n",
    "        \n",
    "        n_real = self.real_b(x_real)\n",
    "        n_im = self.im_b(x_im)  \n",
    "        \n",
    "        output = torch.stack([n_real, n_im], dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Class of upsample block\n",
    "    \"\"\"\n",
    "    def __init__(self, filter_size=(7,5), stride_size=(2,2), in_channels=1, out_channels=45, padding=(0,0)):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.filter_size = filter_size\n",
    "        self.stride_size = stride_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.padding = padding\n",
    "\n",
    "        self.cconv = CConv2d(in_channels=self.in_channels, out_channels=self.out_channels, \n",
    "                             kernel_size=self.filter_size, stride=self.stride_size, padding=self.padding)\n",
    "        \n",
    "        self.cbn = CBatchNorm2d(num_features=self.out_channels) \n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        conved = self.cconv(x)\n",
    "        normed = self.cbn(conved)\n",
    "        acted = self.leaky_relu(normed)\n",
    "        \n",
    "        return acted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Class of downsample block\n",
    "    \"\"\"\n",
    "    def __init__(self, filter_size=(7,5), stride_size=(2,2), in_channels=1, out_channels=45,\n",
    "                 output_padding=(0,0), padding=(0,0), last_layer=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.filter_size = filter_size\n",
    "        self.stride_size = stride_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.output_padding = output_padding\n",
    "        self.padding = padding\n",
    "        \n",
    "        self.last_layer = last_layer\n",
    "        \n",
    "        self.cconvt = CConvTranspose2d(in_channels=self.in_channels, out_channels=self.out_channels, \n",
    "                             kernel_size=self.filter_size, stride=self.stride_size, output_padding=self.output_padding, padding=self.padding)\n",
    "        \n",
    "        self.cbn = CBatchNorm2d(num_features=self.out_channels) \n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        conved = self.cconvt(x)\n",
    "        \n",
    "        if not self.last_layer:\n",
    "            normed = self.cbn(conved)\n",
    "            output = self.leaky_relu(normed)\n",
    "        else:\n",
    "            m_phase = conved / (torch.abs(conved) + 1e-8)\n",
    "            m_mag = torch.tanh(torch.abs(conved))\n",
    "            output = m_phase * m_mag\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCUnet20(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Complex U-Net class of the model.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_fft=64, hop_length=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        # for istft\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        \n",
    "        self.set_size(model_complexity=int(45//1.414), input_channels=1, model_depth=20)\n",
    "        self.encoders = []\n",
    "        self.model_length = 20 // 2\n",
    "        \n",
    "        for i in range(self.model_length):\n",
    "            module = Encoder(in_channels=self.enc_channels[i], out_channels=self.enc_channels[i + 1],\n",
    "                             filter_size=self.enc_kernel_sizes[i], stride_size=self.enc_strides[i], padding=self.enc_paddings[i])\n",
    "            self.add_module(\"encoder{}\".format(i), module)\n",
    "            self.encoders.append(module)\n",
    "\n",
    "        self.decoders = []\n",
    "\n",
    "        for i in range(self.model_length):\n",
    "            if i != self.model_length - 1:\n",
    "                module = Decoder(in_channels=self.dec_channels[i] + self.enc_channels[self.model_length - i], out_channels=self.dec_channels[i + 1], \n",
    "                                 filter_size=self.dec_kernel_sizes[i], stride_size=self.dec_strides[i], padding=self.dec_paddings[i],\n",
    "                                 output_padding=self.dec_output_padding[i])\n",
    "            else:\n",
    "                module = Decoder(in_channels=self.dec_channels[i] + self.enc_channels[self.model_length - i], out_channels=self.dec_channels[i + 1], \n",
    "                                 filter_size=self.dec_kernel_sizes[i], stride_size=self.dec_strides[i], padding=self.dec_paddings[i],\n",
    "                                 output_padding=self.dec_output_padding[i], last_layer=True)\n",
    "            self.add_module(\"decoder{}\".format(i), module)\n",
    "            self.decoders.append(module)\n",
    "       \n",
    "        \n",
    "    def forward(self, x, is_istft=True):\n",
    "        # print('x : ', x.shape)\n",
    "        orig_x = x\n",
    "        xs = []\n",
    "        for i, encoder in enumerate(self.encoders):\n",
    "            xs.append(x)\n",
    "            x = encoder(x)\n",
    "            # print('Encoder : ', x.shape)\n",
    "            \n",
    "        p = x\n",
    "        for i, decoder in enumerate(self.decoders):\n",
    "            p = decoder(p)\n",
    "            if i == self.model_length - 1:\n",
    "                break\n",
    "            # print('Decoder : ', p.shape)\n",
    "            p = torch.cat([p, xs[self.model_length - 1 - i]], dim=1)\n",
    "        \n",
    "        # u9 - the mask\n",
    "        \n",
    "        mask = p\n",
    "        \n",
    "        # print('mask : ', mask.shape)\n",
    "        \n",
    "        output = mask * orig_x\n",
    "        output = torch.squeeze(output, 1)\n",
    "\n",
    "\n",
    "        if is_istft:\n",
    "            output_cmplx = torch.view_as_complex(output)\n",
    "            output = torch.istft(output_cmplx, n_fft=self.n_fft, hop_length=self.hop_length, normalized=True)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    \n",
    "    def set_size(self, model_complexity, model_depth=20, input_channels=1):\n",
    "\n",
    "        if model_depth == 20:\n",
    "            self.enc_channels = [input_channels,\n",
    "                                 model_complexity,\n",
    "                                 model_complexity,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 128]\n",
    "\n",
    "            self.enc_kernel_sizes = [(7, 1),\n",
    "                                     (1, 7),\n",
    "                                     (6, 4),\n",
    "                                     (7, 5),\n",
    "                                     (5, 3),\n",
    "                                     (5, 3),\n",
    "                                     (5, 3),\n",
    "                                     (5, 3),\n",
    "                                     (5, 3),\n",
    "                                     (5, 3)]\n",
    "\n",
    "            self.enc_strides = [(1, 1),\n",
    "                                (1, 1),\n",
    "                                (2, 2),\n",
    "                                (2, 1),\n",
    "                                (2, 2),\n",
    "                                (2, 1),\n",
    "                                (2, 2),\n",
    "                                (2, 1),\n",
    "                                (2, 2),\n",
    "                                (2, 1)]\n",
    "\n",
    "            self.enc_paddings = [(3, 0),\n",
    "                                 (0, 3),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0)]\n",
    "\n",
    "            self.dec_channels = [0,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity,\n",
    "                                 model_complexity,\n",
    "                                 1]\n",
    "\n",
    "            self.dec_kernel_sizes = [(6, 3), \n",
    "                                     (6, 3),\n",
    "                                     (6, 3),\n",
    "                                     (6, 4),\n",
    "                                     (6, 3),\n",
    "                                     (6, 4),\n",
    "                                     (8, 5),\n",
    "                                     (7, 5),\n",
    "                                     (1, 7),\n",
    "                                     (7, 1)]\n",
    "\n",
    "            self.dec_strides = [(2, 1), #\n",
    "                                (2, 2), #\n",
    "                                (2, 1), #\n",
    "                                (2, 2), #\n",
    "                                (2, 1), #\n",
    "                                (2, 2), #\n",
    "                                (2, 1), #\n",
    "                                (2, 2), #\n",
    "                                (1, 1),\n",
    "                                (1, 1)]\n",
    "\n",
    "            self.dec_paddings = [(0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 3),\n",
    "                                 (3, 0)]\n",
    "            \n",
    "            self.dec_output_padding = [(0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0)]\n",
    "        else:\n",
    "            raise ValueError(\"Unknown model depth : {}\".format(model_depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wsdr_fn(x_, y_pred_, y_true_, eps=1e-8):\n",
    "    y_true_cmplx = torch.view_as_complex(y_true_)\n",
    "    # to time-domain waveform\n",
    "    y_true_ = torch.squeeze(y_true_cmplx, 1)\n",
    "    y_true = torch.istft(y_true_, n_fft=N_FFT, hop_length=HOP_LENGTH, normalized=True)\n",
    "    x_ = torch.view_as_complex(x_)\n",
    "    x_ = torch.squeeze(x_, 1)\n",
    "    x = torch.istft(x_, n_fft=N_FFT, hop_length=HOP_LENGTH, normalized=True)\n",
    "\n",
    "    y_pred = y_pred_.flatten(1)\n",
    "    y_true = y_true.flatten(1)\n",
    "    x = x.flatten(1)\n",
    "\n",
    "\n",
    "    def sdr_fn(true, pred, eps=1e-8):\n",
    "        num = torch.sum(true * pred, dim=1)\n",
    "        den = torch.norm(true, p=2, dim=1) * torch.norm(pred, p=2, dim=1)\n",
    "        return -(num / (den + eps))\n",
    "\n",
    "    # true and estimated noise\n",
    "    z_true = x - y_true\n",
    "    z_pred = x - y_pred\n",
    "\n",
    "    a = torch.sum(y_true**2, dim=1) / (torch.sum(y_true**2, dim=1) + torch.sum(z_true**2, dim=1) + eps)\n",
    "    wSDR = a * sdr_fn(y_true, y_pred) + (1 - a) * sdr_fn(z_true, z_pred)\n",
    "    return torch.mean(wSDR)\n",
    "\n",
    "wonky_samples = []\n",
    "\n",
    "def get_metrics_on_loader(loader, net, use_net=True):\n",
    "    net.eval()\n",
    "    # Original test metrics\n",
    "    scale_factor = 32768\n",
    "    # metric_names = [\"CSIG\",\"CBAK\",\"COVL\",\"PESQ\",\"SSNR\",\"STOI\",\"SNR \"]\n",
    "    metric_names = [\"PESQ-WB\",\"PESQ-NB\",\"SNR\",\"SSNR\",\"STOI\"]\n",
    "    overall_metrics = [[] for i in range(5)]\n",
    "    for i, data in enumerate(loader):\n",
    "        if (i+1)%10==0:\n",
    "            end_str = \"\\n\"\n",
    "        else:\n",
    "            end_str = \",\"\n",
    "        #print(i,end=end_str)\n",
    "        if i in wonky_samples:\n",
    "            print(\"Something's up with this sample. Passing...\")\n",
    "        else:\n",
    "            noisy = data[0]\n",
    "            clean = data[1]\n",
    "            noisy_cmplx = torch.view_as_complex(noisy)\n",
    "            clean_cmplx = torch.view_as_complex(clean)\n",
    "            if use_net: # Forward of net returns the istft version\n",
    "                x_est = net(noisy.to(DEVICE), is_istft=True)\n",
    "                x_est_np = x_est.view(-1).detach().cpu().numpy()\n",
    "            else:\n",
    "                x_est_np = torch.istft(torch.squeeze(noisy_cmplx, 1), n_fft=N_FFT, hop_length=HOP_LENGTH, normalized=True).view(-1).detach().cpu().numpy()\n",
    "            x_clean_np = torch.istft(torch.squeeze(clean_cmplx, 1), n_fft=N_FFT, hop_length=HOP_LENGTH, normalized=True).view(-1).detach().cpu().numpy()\n",
    "            \n",
    "        \n",
    "            metrics = AudioMetrics2(x_clean_np, x_est_np, 48000)\n",
    "            \n",
    "            ref_wb = resample(x_clean_np, 48000, 16000)\n",
    "            deg_wb = resample(x_est_np, 48000, 16000)\n",
    "            pesq_wb = pesq(16000, ref_wb, deg_wb, 'wb')\n",
    "            \n",
    "            ref_nb = resample(x_clean_np, 48000, 8000)\n",
    "            deg_nb = resample(x_est_np, 48000, 8000)\n",
    "            pesq_nb = pesq(8000, ref_nb, deg_nb, 'nb')\n",
    "\n",
    "            #print(new_scores)\n",
    "            #print(metrics.PESQ, metrics.STOI)\n",
    "\n",
    "            overall_metrics[0].append(pesq_wb)\n",
    "            overall_metrics[1].append(pesq_nb)\n",
    "            overall_metrics[2].append(metrics.SNR)\n",
    "            overall_metrics[3].append(metrics.SSNR)\n",
    "            overall_metrics[4].append(metrics.STOI)\n",
    "    print()\n",
    "    print(\"Sample metrics computed\")\n",
    "    results = {}\n",
    "    for i in range(5):\n",
    "        temp = {}\n",
    "        temp[\"Mean\"] =  np.mean(overall_metrics[i])\n",
    "        temp[\"STD\"]  =  np.std(overall_metrics[i])\n",
    "        temp[\"Min\"]  =  min(overall_metrics[i])\n",
    "        temp[\"Max\"]  =  max(overall_metrics[i])\n",
    "        results[metric_names[i]] = temp\n",
    "    print(\"Averages computed\")\n",
    "    if use_net:\n",
    "        addon = \"(cleaned by model)\"\n",
    "    else:\n",
    "        addon = \"(pre denoising)\"\n",
    "    print(\"Metrics on test data\",addon)\n",
    "    for i in range(5):\n",
    "        print(\"{} : {:.3f}+/-{:.3f}\".format(metric_names[i], np.mean(overall_metrics[i]), np.std(overall_metrics[i])))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of epoch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net, train_loader, loss_fn, optimizer):\n",
    "    net.train()\n",
    "    train_ep_loss = 0.\n",
    "    counter = 0\n",
    "    for noisy_x, clean_x in train_loader:\n",
    "\n",
    "        noisy_x, clean_x = noisy_x.to(DEVICE), clean_x.to(DEVICE)\n",
    "\n",
    "        # zero  gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        pred_x = net(noisy_x)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = loss_fn(noisy_x, pred_x, clean_x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_ep_loss += loss.item() \n",
    "        counter += 1\n",
    "\n",
    "    train_ep_loss /= counter\n",
    "\n",
    "    # clear cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return train_ep_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(net, test_loader, loss_fn, use_net=True):\n",
    "    net.eval()\n",
    "    test_ep_loss = 0.\n",
    "    counter = 0.\n",
    "    '''\n",
    "    for noisy_x, clean_x in test_loader:\n",
    "        # get the output from the model\n",
    "        noisy_x, clean_x = noisy_x.to(DEVICE), clean_x.to(DEVICE)\n",
    "        pred_x = net(noisy_x)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = loss_fn(noisy_x, pred_x, clean_x)\n",
    "        # Calc the metrics here\n",
    "        test_ep_loss += loss.item() \n",
    "        \n",
    "        counter += 1\n",
    "\n",
    "    test_ep_loss /= counter\n",
    "    '''\n",
    "    \n",
    "    #print(\"Actual compute done...testing now\")\n",
    "    \n",
    "    testmet = getMetricsonLoader(test_loader,net,use_net)\n",
    "\n",
    "    # clear cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return test_ep_loss, testmet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_loader, test_loader, loss_fn, optimizer, scheduler, epochs):\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss = train_epoch(net, train_loader, loss_fn, optimizer)\n",
    "        test_loss = 0\n",
    "        scheduler.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            test_loss, testmet = test_epoch(net, test_loader, loss_fn, use_net=True)\n",
    "\n",
    "        # clear cache\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch + 1}/{epochs}...\",\n",
    "              f\"Train Loss: {train_loss:.6f}...\",\n",
    "                f\"Test Loss: {test_loss:.6f}\")\n",
    "\n",
    "    return train_loss, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clear cache\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "dcunet20 = DCUnet20(N_FFT, HOP_LENGTH).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(dcunet20.parameters())\n",
    "loss_fn = wsdr_fn\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "train_losses, test_losses = train(dcunet20, train_loader, test_loader, loss_fn, optimizer, scheduler, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
